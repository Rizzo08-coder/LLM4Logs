version: '3.8'

services:
  llm4logs:
    build:
      context: ./
      dockerfile: Dockerfile.LLM
    networks:
      - llm4logs
    entrypoint: ["python", "fine_tuning_inference.py"]
    volumes:
      - ./:/llm4logs

  ollama:
    build:
      context: ./
      dockerfile: Dockerfile.ollama
    networks:
      - llm4logs
    ports:
      - '11434:11434'
    volumes:
      - ollama:/root/.ollama
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: 1
    #          capabilities: [gpu]
networks:
  llm4logs:

volumes:
  ollama: